{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "book_df = pd.DataFrame()\n",
    "book_data_list = []\n",
    "\n",
    "\n",
    "def get_book_info(url):\n",
    "    try:\n",
    "\n",
    "        res = requests.get(url)\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)  \n",
    "        time.sleep(2)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        target = soup.find('div', class_='meta-data')\n",
    "        \n",
    "        b_name_element = target.find('p', class_='book-name')\n",
    "        b_name = b_name_element.text.strip() if b_name_element else None\n",
    "\n",
    "        b_author_element = soup.find('p', class_='author')\n",
    "        b_author = b_author_element.find('span').text.strip() if b_author_element else None\n",
    "        \n",
    "        b_readtogether_element = soup.find('div', class_='read-together')\n",
    "        b_readtogether = b_readtogether_element.find('strong',class_='number').text.strip() if b_readtogether_element else None\n",
    "\n",
    "        category_data = soup.find('div', class_='book-info-detail slide-container')\n",
    "        if category_data:\n",
    "            b_category = category_data.find('a')\n",
    "            if b_category:\n",
    "                b_category = b_category.text.strip()\n",
    "            else:\n",
    "                b_category = 'no'\n",
    "        else:\n",
    "            b_category = 'no'\n",
    "        \n",
    "        b_percent_element = soup.find('strong', class_='line-desc')\n",
    "        b_percent = b_percent_element.text.strip() if b_percent_element else None\n",
    "        \n",
    "        # b_reviewcount_element = soup.find('div', class_='review')\n",
    "        # b_reviewcount = b_reviewcount_element.find('strong',class_='number').text.strip() if b_reviewcount_element else None \n",
    "        b_reviewcount_element = soup.find('div', class_='review')\n",
    "\n",
    "        if b_reviewcount_element:\n",
    "            b_reviewcount = b_reviewcount_element.find('strong', class_='number')\n",
    "            if b_reviewcount:\n",
    "                b_reviewcount = b_reviewcount.text.strip()\n",
    "            else:\n",
    "                b_reviewcount = 0\n",
    "        else:\n",
    "            b_reviewcount = 0\n",
    "\n",
    "        b_keyword_element = soup.find('div', class_='keyword-divider')\n",
    "        b_keyword = b_keyword_element.find('a', class_='keyword').text.strip() if b_keyword_element else None\n",
    "        \n",
    "        \n",
    "        # 오디오북 여부\n",
    "        try:\n",
    "            b_type = soup.find('div', class_='book-type')\n",
    "            if b_type:\n",
    "                b_audiobook = \"O\"\n",
    "            else:\n",
    "                b_audiobook = \"X\"\n",
    "        except AttributeError:\n",
    "            b_audiobook = \"X\"\n",
    "\n",
    "        book_data = []\n",
    "        book_data.append({'Book_Name': b_name, 'Book_Author': b_author,'Book_Category': b_category, 'Completion_Percent': b_percent, \n",
    "                        'ReadTogether': b_readtogether, 'ReviewCount':b_reviewcount ,'Keyword' : b_keyword,'AudioBook' : b_audiobook})\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return  book_data,{'Book_Name': b_name}\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "\n",
    "def get_reviews(url):\n",
    "    try:\n",
    "\n",
    "        res = requests.get(url)\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)  # Open the URL\n",
    "        time.sleep(2)  # Add a delay to allow the page to load (you can adjust the time as needed)\n",
    "        \n",
    "        # Check if there are reviews\n",
    "        if \"리뷰가 없습니다.\" in driver.page_source:\n",
    "            print(\"No reviews for this book. Skipping...\")\n",
    "            driver.quit()\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Scroll to the end of the page\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Add a delay after scrolling\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        target = soup.find('ul', class_='review-list')\n",
    "        p_names = target.find_all('p', class_='nickname')\n",
    "        p_texts = target.find_all('pre', class_='cont')\n",
    "\n",
    "        reviews_data = []\n",
    "\n",
    "        for p_name, p_text in zip(p_names, p_texts):\n",
    "            a_tag_text = p_name.find('a').text\n",
    "            reviews_data.append({'Nickname': a_tag_text, 'Review_Text': p_text.text})\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for URL {url}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "base_url = \"https://www.millie.co.kr/v3/bookDetail/{}\"\n",
    "\n",
    "start_number = 179627456\n",
    "end_number = 179627798\n",
    "\n",
    "for book_number in range(start_number, end_number + 1):\n",
    "    \n",
    "    book_url = base_url.format(book_number)\n",
    "    book_data, book_info = get_book_info(book_url)\n",
    "   \n",
    "    # 등록되지 않은 번호이면 book_data, book_info가 None으로 반환될 것이고, 이 경우에는 건너뛰고 다음 번호로 진행\n",
    "    if book_data is not None and book_info is not None:\n",
    "        book_df = pd.concat([book_df, pd.DataFrame(book_data)])\n",
    "\n",
    "        review_url = f\"https://www.millie.co.kr/v3/bookDetail/more/review/{book_number}\"\n",
    "        reviews_df = get_reviews(review_url)\n",
    "\n",
    "        # 리뷰가 있는 경우에만 데이터 수집\n",
    "        if not reviews_df.empty:\n",
    "            # 책 정보를 리뷰 데이터프레임에 추가\n",
    "            reviews_df['Book_Name'] = book_info['Book_Name']\n",
    "\n",
    "            # CSV 파일로 저장\n",
    "            reviews_df.to_csv(f'All_book_reviews_{book_number}.csv', index=False, encoding='utf-8')\n",
    "\n",
    "            print(f\"Data for book number {book_number} saved successfully.\")\n",
    "        else:\n",
    "            print(f\"No reviews for book number {book_number}. Skipping...\")\n",
    "\n",
    "book_df.to_csv('Bookdata_.csv', index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
